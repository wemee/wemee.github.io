# Stairs RL Training Experiments Log

記錄所有 RL 訓練實驗的配置、結果和觀察，用於追蹤改進方向。

---

## Experiment #1: Baseline (舊參數)

**日期**: 2026-01-14
**目的**: 初始訓練，使用預設配置

### 配置
- **遊戲參數**:
  - 初始 Y 位置: 100
  - 彈跳速度: -12
- **觀察空間**: 54 維
  - 玩家: 4 維 (x, y, vx, vy)
  - 樓梯: 10 × 5 = 50 維（**包含上下所有樓梯**，按距離排序）
- **獎勵函數**:
  - 死亡: -100
  - 存活: +0.1 /步
  - 得分: +10 /分
  - 位置獎勵: 0~0.5（鼓勵在 y=360）
- **模型架構**: PPO, MlpPolicy [64, 64], Tanh
- **超參數**:
  - learning_rate: 3e-4
  - n_steps: 2048
  - batch_size: 64
  - n_epochs: 10
  - gamma: 0.99
  - clip_range: 0.2
  - ent_coef: 0.01

### 訓練
- **總步數**: 500,000
- **並行環境**: 4

### 結果
- **最終平均分數**: 未記錄（訓練完成但未評估）
- **模型輸出**: `output/models/final_model.zip`, `model_weights.json`

### 觀察
- 訓練完成，模型已部署到前端
- 未進行系統性評估

---

## Experiment #2: 新參數 + 初步測試

**日期**: 2026-01-15 10:14
**目的**: 測試新遊戲參數（降低難度）+ 快速驗證訓練流程

### 配置
- **遊戲參數** ⚠️ 變更:
  - 初始 Y 位置: 100 → **300**
  - 彈跳速度: -12 → **-6**
- **觀察空間**: 54 維（同 Exp #1）
  - ⚠️ **問題**: 包含上方樓梯（無用資訊）
- **獎勵函數**: 同 Exp #1
  - ⚠️ **問題**:
    - 存活獎勵太低 (0.1)
    - 得分獎勵稀疏 (10)
    - 位置獎勵可能鼓勵「停留」
- **模型架構**: 同 Exp #1
- **超參數**: 同 Exp #1

### 訓練
- **總步數**: 5,000 → 實際 8,192 步
- **並行環境**: 4
- **訓練時長**: ~12 秒
- **FPS**: 636 步/秒

### 結果
| 指標 | 4K 步 | 8K 步 | 最終評估 |
|------|-------|-------|----------|
| 平均獎勵 | 50.1 ± 43.5 | **22.7 ± 48.6** | — |
| 平均分數 | ？ | ？ | **4.8 ± 3.8** |
| Episode 長度 | 232 ± 52 | 192 ± 70 | 264 ± 122 |

**訓練期間指標**:
- ep_len_mean: 245
- ep_rew_mean: 68.4

### 觀察 ❌
- **評估獎勵持續下降**: 50.1 → 22.7（-55%）
- **最終表現極差**: 平均分數僅 4.8，最高 13
- **高變異性**: 標準差大，表現不穩定
- **可能進入 Local Optimal**: AI 可能學會「原地不動」避免死亡

### 問題診斷
1. **觀察空間污染**: 包含無用的上方樓梯
2. **獎勵設計缺陷**:
   - 存活獎勵 (0.1) vs 死亡懲罰 (-100) 比例失衡
   - 得分獎勵稀疏，初期難以學習
   - 位置獎勵可能干擾，鼓勵停留而非移動
3. **訓練步數太少**: 8K 步不足以學到複雜策略

### 結論
需要根本性改進觀察空間和獎勵函數。

---

## Experiment #3: 觀察空間修正 + 獎勵方案 A

**日期**: 2026-01-15
**目的**: 修正觀察空間（只含下方樓梯）+ 激進獎勵改進

### 配置變更
- **遊戲參數**: 同 Exp #2（初始 Y=300, 彈跳=-6）
- **觀察空間** ✅ 改進:
  - ⚠️ **只包含玩家下方的樓梯**（y >= player.y）
  - 按距離由近到遠排序（不使用絕對值）
  - 減少無用資訊，降低學習難度
- **獎勵函數** ✅ 方案 A（激進改進）:
  ```typescript
  死亡: -100 + min(score * 2, 50)  // 存活越久懲罰越小
  得分: +50 (from +10)             // 5x 提升
  存活: +1.0 (from +0.1)           // 10x 提升
  移動: +0.5 (新增)                // 鼓勵主動下落
  位置: 移除                       // 避免鼓勵停留
  ```
- **模型架構**: 同 Exp #1
- **超參數**: 同 Exp #1

### 訓練
- **總步數**: 5,000
- **並行環境**: 4
- **評估頻率**: 1000 步（更頻繁監控）

### 訓練
- **總步數**: 5,000 → 實際 8,192 步
- **並行環境**: 4
- **訓練時長**: ~8 秒
- **FPS**: 940 步/秒 (提升自 636)

### 結果
| 指標 | 4K 步 | 8K 步 | 最終評估 | Exp #2 對比 |
|------|-------|-------|----------|------------|
| 平均獎勵 | 77.2 ± 179.7 | **275.1 ± 283.6** ✅ | — | 22.7 (12x 提升！) |
| 平均分數 | ？ | ？ | **1.4 ± 2.0** | 4.8 (反而下降) |
| Episode 長度 | 98 ± 78 | **171 ± 104** | 115 ± 85 | 192 |

**訓練期間指標**:
- ep_len_mean: 283 (vs 245 in Exp #2)
- ep_rew_mean: **543** (vs 68.4 in Exp #2) ✅ 8x 提升！

**最佳 Episode**:
- 最高分數: 6 (vs 13 in Exp #2)
- 最長存活: 288 步，獲得 519 獎勵

### 觀察 ✅ 部分成功
- **評估獎勵穩定上升**: 77.2 → 275.1 (+256%) ✅ 解決了 Exp #2 的下降問題！
- **訓練獎勵大幅提升**: 543 vs 68.4 (+694%) ✅
- **Episode 長度增加**: 283 vs 245 ✅
- **但分數反而下降**: 1.4 vs 4.8 ⚠️

### 問題診斷
**矛盾現象**: 獎勵大幅提升，但分數下降

**可能原因**:
1. **獎勵函數側重存活而非得分**:
   - 存活獎勵 +1.0/步，移動獎勵 +0.5/步
   - 100 步 = 150 獎勵（存活+移動）
   - 1 分 = 50 獎勵
   - **存活 100 步 > 得 3 分**（就獎勵而言）
   - AI 學會「盡量活著」而非「積極得分」

2. **5K 步訓練仍太短**:
   - AI 可能還在學習基礎生存
   - 尚未學到「得分」的複雜策略

3. **觀察空間改進有效**:
   - 獎勵穩定上升說明學習方向正確
   - 只是獎勵權重需要調整

### 結論
✅ **改進有效**: 觀察空間修正 + 獎勵重新設計 = 學習穩定性大幅提升
⚠️ **需要微調**: 獎勵權重偏向存活，需要提高得分獎勵比重

### 下一步建議
1. **調整獎勵權重** (優先):
   - 得分獎勵: 50 → 100 (進一步提升)
   - 存活獎勵: 1.0 → 0.5 (降低)
   - 移動獎勵: 0.5 → 0.2 (降低)
   - 讓「得分」成為絕對主要目標

2. **增加訓練步數**:
   - 5K → 20K-50K 看是否能學會得分策略

3. **監控分數趨勢**:
   - 如果分數持續不上升，考慮 shaped reward
   - 例如：接近樓梯也給小獎勵

---

## 後續實驗方向

### 如果 Exp #3 成功
- 增加訓練步數到 50K（加 Early Stopping）
- 微調獎勵權重
- 嘗試增加 n_envs 加速訓練

### 如果 Exp #3 失敗
- 回退到獎勵方案 B（保守改進）
- 調整 PPO 超參數（learning_rate, clip_range）
- 考慮 curriculum learning（漸進式難度）

---

## 實驗追蹤 Checklist

每次實驗記錄：
- [ ] 日期、實驗編號
- [ ] 配置變更（遊戲參數、觀察空間、獎勵、超參數）
- [ ] 訓練設定（步數、環境數、時長）
- [ ] 結果指標（獎勵、分數、Episode 長度）
- [ ] 觀察和問題診斷
- [ ] 結論和下一步
