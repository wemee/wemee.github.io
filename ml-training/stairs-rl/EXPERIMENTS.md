# Stairs RL Training Experiments Log

記錄所有 RL 訓練實驗的配置、結果和觀察，用於追蹤改進方向。

---

## Experiment #1: Baseline (舊參數)

**日期**: 2026-01-14
**目的**: 初始訓練，使用預設配置

### 配置
- **遊戲參數**:
  - 初始 Y 位置: 100
  - 彈跳速度: -12
- **觀察空間**: 54 維
  - 玩家: 4 維 (x, y, vx, vy)
  - 樓梯: 10 × 5 = 50 維（**包含上下所有樓梯**，按距離排序）
- **獎勵函數**:
  - 死亡: -100
  - 存活: +0.1 /步
  - 得分: +10 /分
  - 位置獎勵: 0~0.5（鼓勵在 y=360）
- **模型架構**: PPO, MlpPolicy [64, 64], Tanh
- **超參數**:
  - learning_rate: 3e-4
  - n_steps: 2048
  - batch_size: 64
  - n_epochs: 10
  - gamma: 0.99
  - clip_range: 0.2
  - ent_coef: 0.01

### 訓練
- **總步數**: 500,000
- **並行環境**: 4

### 結果
- **最終平均分數**: 未記錄（訓練完成但未評估）
- **模型輸出**: `output/models/final_model.zip`, `model_weights.json`

### 觀察
- 訓練完成，模型已部署到前端
- 未進行系統性評估

---

## Experiment #2: 新參數 + 初步測試

**日期**: 2026-01-15 10:14
**目的**: 測試新遊戲參數（降低難度）+ 快速驗證訓練流程

### 配置
- **遊戲參數** ⚠️ 變更:
  - 初始 Y 位置: 100 → **300**
  - 彈跳速度: -12 → **-6**
- **觀察空間**: 54 維（同 Exp #1）
  - ⚠️ **問題**: 包含上方樓梯（無用資訊）
- **獎勵函數**: 同 Exp #1
  - ⚠️ **問題**:
    - 存活獎勵太低 (0.1)
    - 得分獎勵稀疏 (10)
    - 位置獎勵可能鼓勵「停留」
- **模型架構**: 同 Exp #1
- **超參數**: 同 Exp #1

### 訓練
- **總步數**: 5,000 → 實際 8,192 步
- **並行環境**: 4
- **訓練時長**: ~12 秒
- **FPS**: 636 步/秒

### 結果
| 指標 | 4K 步 | 8K 步 | 最終評估 |
|------|-------|-------|----------|
| 平均獎勵 | 50.1 ± 43.5 | **22.7 ± 48.6** | — |
| 平均分數 | ？ | ？ | **4.8 ± 3.8** |
| Episode 長度 | 232 ± 52 | 192 ± 70 | 264 ± 122 |

**訓練期間指標**:
- ep_len_mean: 245
- ep_rew_mean: 68.4

### 觀察 ❌
- **評估獎勵持續下降**: 50.1 → 22.7（-55%）
- **最終表現極差**: 平均分數僅 4.8，最高 13
- **高變異性**: 標準差大，表現不穩定
- **可能進入 Local Optimal**: AI 可能學會「原地不動」避免死亡

### 問題診斷
1. **觀察空間污染**: 包含無用的上方樓梯
2. **獎勵設計缺陷**:
   - 存活獎勵 (0.1) vs 死亡懲罰 (-100) 比例失衡
   - 得分獎勵稀疏，初期難以學習
   - 位置獎勵可能干擾，鼓勵停留而非移動
3. **訓練步數太少**: 8K 步不足以學到複雜策略

### 結論
需要根本性改進觀察空間和獎勵函數。

---

## Experiment #3: 觀察空間修正 + 獎勵方案 A

**日期**: 2026-01-15
**目的**: 修正觀察空間（只含下方樓梯）+ 激進獎勵改進

### 配置變更
- **遊戲參數**: 同 Exp #2（初始 Y=300, 彈跳=-6）
- **觀察空間** ✅ 改進:
  - ⚠️ **只包含玩家下方的樓梯**（y >= player.y）
  - 按距離由近到遠排序（不使用絕對值）
  - 減少無用資訊，降低學習難度
- **獎勵函數** ✅ 方案 A（激進改進）:
  ```typescript
  死亡: -100 + min(score * 2, 50)  // 存活越久懲罰越小
  得分: +50 (from +10)             // 5x 提升
  存活: +1.0 (from +0.1)           // 10x 提升
  移動: +0.5 (新增)                // 鼓勵主動下落
  位置: 移除                       // 避免鼓勵停留
  ```
- **模型架構**: 同 Exp #1
- **超參數**: 同 Exp #1

### 訓練
- **總步數**: 5,000
- **並行環境**: 4
- **評估頻率**: 1000 步（更頻繁監控）

### 結果
待執行...

### 預期改進
- 評估獎勵應該穩定上升（不再下降）
- 平均分數 > 10
- Episode 長度增加
- 變異性降低

---

## 後續實驗方向

### 如果 Exp #3 成功
- 增加訓練步數到 50K（加 Early Stopping）
- 微調獎勵權重
- 嘗試增加 n_envs 加速訓練

### 如果 Exp #3 失敗
- 回退到獎勵方案 B（保守改進）
- 調整 PPO 超參數（learning_rate, clip_range）
- 考慮 curriculum learning（漸進式難度）

---

## 實驗追蹤 Checklist

每次實驗記錄：
- [ ] 日期、實驗編號
- [ ] 配置變更（遊戲參數、觀察空間、獎勵、超參數）
- [ ] 訓練設定（步數、環境數、時長）
- [ ] 結果指標（獎勵、分數、Episode 長度）
- [ ] 觀察和問題診斷
- [ ] 結論和下一步
